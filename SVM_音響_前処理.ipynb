{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM_音響_前処理.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOtFVQnseTADt2Ua18I+hdd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"sTb00nDkLHyj","executionInfo":{"status":"ok","timestamp":1643786645724,"user_tz":-420,"elapsed":862,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"}}},"outputs":[],"source":["#インポート\n","if 1:\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import cv2\n","  import os\n","  import pickle\n","  from google.colab import drive\n","  from google.colab.patches import cv2_imshow\n","  from scipy import io\n","  from math import *\n","  from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WXjTvAv_LRXL","executionInfo":{"status":"ok","timestamp":1643786670726,"user_tz":-420,"elapsed":25005,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"}},"outputId":"cdca869d-48af-4a3c-e461-94a9be5ea373"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#正規化\n","def z_score(t,data):\n","  out=np.zeros([len(data),len(data[0]),len(data[0][0])])\n","  r0=0\n","  r1=0\n","\n","  data=data.transpose(1,0,2)\n","  data-=np.sum(data,axis=0)/len(data)#平均０\n","  data/=np.sqrt(np.sum(data*data,axis=0)/len(data))#分散１\n","  data=data.transpose(1,0,2)\n","  out=data\n","  #plt.show()\n","  return t,out\n","\n","#最小二乗法によるベースライン補正\n","def base_line(t,data):\n","  data=data.transpose(0,2,1)\n","\n","  a=np.zeros([2,len(data),len(data[0])])\n","  out=np.zeros([len(data),len(data[0]),len(data[0][0])])\n","\n","  A=np.sum(data*t,axis=2)/len(t)#sum_xy\n","  B=np.sum(data,axis=2)/len(t)#sum_y\n","  C=np.sum(t*t,axis=0)/len(t)#sum_xx\n","  D=np.sum(t,axis=0)/len(t)#sum_x\n","\n","  a[0]=(B*C-A*D)/(C-D*D)\n","  a[1]=(A-B*D)/(C-D*D)\n","\n","  data=data.transpose(2,0,1)\n","  out=out.transpose(2,0,1)\n","\n","  for time in range(len(t)):\n","    out[time]=data[time]-(a[0]+a[1]*t[time])\n","  \n","  data=data.transpose(1,0,2)\n","  out=out.transpose(1,0,2)\n","  \n","  return t,out\n","\n","#フーリエ変換\n","def fourier(t,data):\n","  n_sam=1500\n","  out=np.zeros([len(data),len(data[0]),len(data[0][0])])\n","  \n","  F=(np.fft.fft(data,n_sam,axis=1,norm=\"ortho\")).real\n","  freq=np.fft.fftfreq(n_sam,1/len(t))\n","\n","  F=F.transpose(1,0,2)\n","  F=F[0:floor(8/len(t)*n_sam)]\n","  F=F.transpose(1,0,2)\n","\n","  freq=freq[0:floor(8/len(t)*n_sam)]\n","\n","  return freq,F\n","\n","#試行加算平均（セッション別）\n","def ave_per_ses(data,label,num_ses,n_class):\n","  m_ses=12\n","  out_data=np.zeros([n_class*m_ses,len(data[0]),len(data[0][0])])\n","  n_label=np.zeros([n_class*m_ses])\n","\n","  for trial in range(len(data)):#データ\n","    if n_class==2:\n","      ind=(label[trial][0]+1)/2\n","    else:\n","      ind=label[trial][0]-1\n","      \n","    ind=int(ind+n_class*(num_ses[trial][0]-1))\n","    n_label[ind]+=1\n","    out_data[ind]+=data[trial]\n","  \n","  out_data=out_data.transpose(2,1,0)\n","  out_data/=n_label\n","  out_data=out_data.transpose(2,1,0)\n","\n","  out_label=np.zeros([n_class*m_ses,1])#ラベル\n","  for trial in range(n_class*m_ses):\n","    if n_class==2:\n","      out_label[trial]=(trial%n_class)*2-1\n","    else:\n","      out_label[trial]=(trial%n_class)+1\n","  return out_data,out_label\n","\n","# ラベル変形\n","def remake_label(label,n_class):\n","  label2=np.zeros([len(label),n_class])\n","\n","  \n","  for n_label in range(len(label)):\n","    if n_class==2:\n","      cl=int((label[n_label]+1)/2)\n","    else:\n","      cl=int(abs(label[n_label]-1))\n","    label2[n_label][cl]=1\n","\n","  return label2\n","\n","#データセット編集(ロード時)\n","def arange_data(data_prepro,label,num_ses,n_class):\n","  t=np.arange(0,1,1/256)\n","  t,data=base_line(t,data_prepro)\n","  t,data=z_score(t,data)\n","  #freq,data=fourier(t,data)\n","  data,label=ave_per_ses(data,label,num_ses,n_class)\n","  ##label=remake_label(label,n_class)\n","  \n","  #data,label=sub_trial_ave(data,label)\n","  #data=del_ch_ave(data)\n","  #print(data.shape,label.shape)\n","  #t,data=z_score(t,data)\n","  return data,label\n","\n","def make_dataset(n_class):\n","  #データロード\n","  after_data = []\n","  after_label = []\n","  for people in range(1,10):#max=10\n","    matdata_org = io.loadmat(\"/content/drive/Shareddrives/和田・南部研究室共有ドライブ/datasets/eeg_sound/s\"+str(people)+\"_500.mat\")# loading data (for 500ms SOA)\n","    \n","    data_prepro =matdata_org['DataF']# pre-proceed data (subtrials x time x channels)\n","    label_code =matdata_org['Code']# sound direction (1-6, labels for 6-class classification)\n","    label_target =matdata_org['Group'] #target-nontarget labels (1:target, -1:nontarget)\n","    #num_rep = matdata_org['Rep']# repetition number (= trial number)\n","    num_ses = matdata_org['Session']# Session number (= session number)\n","\n","    if n_class==2:\n","      label_0=label_target\n","    else:\n","      label_0=label_code\n","\n","    if people ==1:\n","      data_1,label_1=arange_data(data_prepro,label_0,num_ses,n_class)\n","    else:\n","      data_0,label_0=arange_data(data_prepro,label_0,num_ses,n_class)\n","      data_1=np.concatenate([data_1,data_0])\n","      label_1=np.concatenate([label_1,label_0])\n","    #print(data_1.shape,label_1.shape)\n","    after_data.append(data_1)\n","    after_label.append(label_1)\n","  #データセット編集（ロード後)\n","  data,label=data_1,label_1\n","  t=np.arange(0,1,1/256)\n","  #t,data=base_line(t,data_prepro)\n","  #t,data=z_score(t,data)\n","  freq,data=fourier(t,data)\n","  #data,label=sub_trial_ave(data,label)\n","  #data=del_ch_ave(data)\n","  #print(data.shape,label.shape)\n","  t,data=z_score(t,data)\n","\n","  return after_data,after_label"],"metadata":{"id":"0V5wZLwRLSQM","executionInfo":{"status":"ok","timestamp":1643786671587,"user_tz":-420,"elapsed":865,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["data,label=make_dataset(2)#output preprocessed data and label\n","data1,data2,data3,data4,data5,data6,data7,data8,data9 = data\n","label1,label2,label3,label4,label5,label6,label7,label8,label9 = label\n","D  = [data1,data2,data3,data4,data5,data6,data7,data8,data9]\n","L = [label1,label2,label3,label4,label5,label6,label7,label8,label9]\n"],"metadata":{"id":"QEs6LaRCLvq8","executionInfo":{"status":"ok","timestamp":1643786732843,"user_tz":-420,"elapsed":61258,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(D[0].shape)\n","datax = D[0].reshape(D[0].shape[0],D[0].shape[1]*D[0].shape[2])\n","print(datax.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mtZ3Q78CzOWX","executionInfo":{"status":"ok","timestamp":1643786732843,"user_tz":-420,"elapsed":7,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"}},"outputId":"ce7251d9-6bb1-4df9-c6f4-79faddf8b6f2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["(24, 256, 64)\n","(24, 16384)\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12467,"status":"ok","timestamp":1643786745307,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"},"user_tz":-420},"id":"C08V6LkuYlqp","outputId":"d5e49a30-59ea-483b-996a-c6dc0b9f8e53"},"outputs":[{"output_type":"stream","name":"stdout","text":["S1_500.mat\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 0.625\n","Model accuracy (testing)              : 0.7\n","Model accuracy (testing)              : 0.75\n","Max accuracy Test : 1.0\n","Best Percentage of test data % : 0.1\n","-------------------------------------\n","S2_500.mat\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 0.8666666666666667\n","Model accuracy (testing)              : 0.9\n","Model accuracy (testing)              : 0.9166666666666666\n","Max accuracy Test : 1.0\n","Best Percentage of test data % : 0.1\n","-------------------------------------\n","S3_500.mat\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 0.75\n","Max accuracy Test : 1.0\n","Best Percentage of test data % : 0.1\n","-------------------------------------\n","S4_500.mat\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 0.8275862068965517\n","Model accuracy (testing)              : 0.9230769230769231\n","Model accuracy (testing)              : 0.9375\n","Max accuracy Test : 1.0\n","Best Percentage of test data % : 0.1\n","-------------------------------------\n","S5_500.mat\n","Model accuracy (testing)              : 0.9166666666666666\n","Model accuracy (testing)              : 0.875\n","Model accuracy (testing)              : 0.8333333333333334\n","Model accuracy (testing)              : 0.8125\n","Model accuracy (testing)              : 0.85\n","Max accuracy Test : 0.9166666666666666\n","Best Percentage of test data % : 0.1\n","-------------------------------------\n","S6_500.mat\n","Model accuracy (testing)              : 1.0\n","Model accuracy (testing)              : 0.8620689655172413\n","Model accuracy (testing)              : 0.9090909090909091\n","Model accuracy (testing)              : 0.8793103448275862\n","Model accuracy (testing)              : 0.9027777777777778\n","Max accuracy Test : 1.0\n","Best Percentage of test data % : 0.1\n","-------------------------------------\n","S7_500.mat\n","Model accuracy (testing)              : 0.8823529411764706\n","Model accuracy (testing)              : 0.9411764705882353\n","Model accuracy (testing)              : 0.9411764705882353\n","Model accuracy (testing)              : 0.9558823529411765\n","Model accuracy (testing)              : 0.9166666666666666\n","Max accuracy Test : 0.9558823529411765\n","Best Percentage of test data % : 0.4\n","-------------------------------------\n","S8_500.mat\n","Model accuracy (testing)              : 0.95\n","Model accuracy (testing)              : 0.9487179487179487\n","Model accuracy (testing)              : 0.9310344827586207\n","Model accuracy (testing)              : 0.935064935064935\n","Model accuracy (testing)              : 0.8958333333333334\n","Max accuracy Test : 0.95\n","Best Percentage of test data % : 0.1\n","-------------------------------------\n","S9_500.mat\n","Model accuracy (testing)              : 0.9090909090909091\n","Model accuracy (testing)              : 0.9545454545454546\n","Model accuracy (testing)              : 0.9230769230769231\n","Model accuracy (testing)              : 0.9195402298850575\n","Model accuracy (testing)              : 0.9259259259259259\n","Max accuracy Test : 0.9545454545454546\n","Best Percentage of test data % : 0.2\n","-------------------------------------\n"]}],"source":["from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from sklearn import tree\n","from sklearn.metrics import accuracy_score\n","def split(data,label):  #Use Decision Tree to calculate test size optimally.\n","  a = np.array([])\n","  K = [0.1, 0.2, 0.3, 0.4, 0.5]\n","  datax = data.reshape(data.shape[0],data.shape[1]*data.shape[2])\n","  labelx = label.flatten()\n","  for test_split in K:\n","    #print(\"Percentage of test data in datasets % :\", test_split)\n","    X_train, X_test, y_train, y_test = train_test_split(datax, labelx, test_size= test_split, random_state=0)\n","    #print(\"Train data shape                      :\", X_train.shape, y_train.shape)\n","    #print(\"           label counts               :\", Counter(y_train))\n","    #print(\"Test  data shape                      :\", X_test.shape , y_test.shape)\n","    #print(\"           label counts               :\", Counter(y_test))\n","    \n","    clf = tree.DecisionTreeClassifier(random_state=0)\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_train)\n","    #print(\"Model accuracy (training)             :\", accuracy_score(y_train, y_pred))\n","\n","    # make predictions using test data\n","    y_test_pred = clf.predict(X_test)\n","    a = np.append(a, accuracy_score(y_test, y_test_pred))\n","    # calculate test data accuracy\n","    print(\"Model accuracy (testing)              :\", accuracy_score(y_test, y_test_pred))\n","    \n","  print('Max accuracy Test :',max(a))\n","  i = int(np.where(a == max(a))[0][0])\n","  print('Best Percentage of test data % :',K[i])\n","  print('-------------------------------------')\n","  return K[i]\n","spl1= []\n","for i in range(0,9):\n","  print('S'+str(i+1)+'_500.mat')\n","  spl = split(D[i],L[i])\n","  spl1.append(spl)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"DOTO9K3AS0Hx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643786745307,"user_tz":-420,"elapsed":6,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"}},"outputId":"735c0cd2-b67f-4751-9487-afa6daac7427"},"outputs":[{"output_type":"stream","name":"stdout","text":["(43,)\n","(43, 16384)\n"]}],"source":["X_train = []\n","y_train = []\n","X_test = []\n","y_test = []\n","for i in range(0,9):\n","  #Create Train dataset and Test dataset with Test_size = spl\n","  datax = D[i].reshape(D[i].shape[0],D[i].shape[1]*D[i].shape[2])\n","  labelx = L[i].flatten()\n","  X_tr, X_t, y_tr, y_t = train_test_split(datax, labelx, test_size= spl1[i], random_state=0)\n","  X_train.append(X_tr)\n","  X_test.append(X_t)\n","  y_train.append(y_tr)\n","  y_test.append(y_t)\n","print(y_train[1].shape)\n","print(X_train[1].shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoaZFM_kffNB","outputId":"ddd4649e-2835-489e-cfc6-c93dfb95aa2e","executionInfo":{"status":"ok","timestamp":1643786754174,"user_tz":-420,"elapsed":8871,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["S1_500.mat\n","   mean_test_score  std_test_score                           params\n","0             0.96            0.08     {'C': 1, 'kernel': 'linear'}\n","1             0.96            0.08    {'C': 10, 'kernel': 'linear'}\n","2             0.96            0.08   {'C': 100, 'kernel': 'linear'}\n","3             0.96            0.08  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  0.96\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","S2_500.mat\n","   mean_test_score  std_test_score                           params\n","0              1.0             0.0     {'C': 1, 'kernel': 'linear'}\n","1              1.0             0.0    {'C': 10, 'kernel': 'linear'}\n","2              1.0             0.0   {'C': 100, 'kernel': 'linear'}\n","3              1.0             0.0  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  1.0\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","S3_500.mat\n","   mean_test_score  std_test_score                           params\n","0              1.0             0.0     {'C': 1, 'kernel': 'linear'}\n","1              1.0             0.0    {'C': 10, 'kernel': 'linear'}\n","2              1.0             0.0   {'C': 100, 'kernel': 'linear'}\n","3              1.0             0.0  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  1.0\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","S4_500.mat\n","   mean_test_score  std_test_score                           params\n","0          0.94183        0.037226     {'C': 1, 'kernel': 'linear'}\n","1          0.94183        0.037226    {'C': 10, 'kernel': 'linear'}\n","2          0.94183        0.037226   {'C': 100, 'kernel': 'linear'}\n","3          0.94183        0.037226  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  0.9418300653594771\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","S5_500.mat\n","   mean_test_score  std_test_score                           params\n","0         0.943723        0.045977     {'C': 1, 'kernel': 'linear'}\n","1         0.943723        0.045977    {'C': 10, 'kernel': 'linear'}\n","2         0.943723        0.045977   {'C': 100, 'kernel': 'linear'}\n","3         0.943723        0.045977  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  0.9437229437229437\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","S6_500.mat\n","   mean_test_score  std_test_score                           params\n","0         0.914769        0.061474     {'C': 1, 'kernel': 'linear'}\n","1         0.914769        0.061474    {'C': 10, 'kernel': 'linear'}\n","2         0.914769        0.061474   {'C': 100, 'kernel': 'linear'}\n","3         0.914769        0.061474  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  0.9147692307692308\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","S7_500.mat\n","   mean_test_score  std_test_score                           params\n","0             0.91        0.073485     {'C': 1, 'kernel': 'linear'}\n","1             0.91        0.073485    {'C': 10, 'kernel': 'linear'}\n","2             0.91        0.073485   {'C': 100, 'kernel': 'linear'}\n","3             0.91        0.073485  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  0.9099999999999999\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","S8_500.mat\n","   mean_test_score  std_test_score                           params\n","0          0.96521        0.042616     {'C': 1, 'kernel': 'linear'}\n","1          0.96521        0.042616    {'C': 10, 'kernel': 'linear'}\n","2          0.96521        0.042616   {'C': 100, 'kernel': 'linear'}\n","3          0.96521        0.042616  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  0.9652100840336134\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","S9_500.mat\n","   mean_test_score  std_test_score                           params\n","0         0.936303        0.027659     {'C': 1, 'kernel': 'linear'}\n","1         0.936303        0.027659    {'C': 10, 'kernel': 'linear'}\n","2         0.936303        0.027659   {'C': 100, 'kernel': 'linear'}\n","3         0.936303        0.027659  {'C': 1000, 'kernel': 'linear'}\n","\n","Max_score =  0.9363025210084034\n","Estimator :  SVC(C=1, kernel='linear')\n","-------------------------------------\n","Cross validation accuracy:  95.242609 %\n"]}],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","import pandas as pd\n","def parameters(X_train1,y_train1):  #Finding the best Hyperparameter tuning in SVM (in case the kernel is linear)\n","  param_grid = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']}]\n","  #print(param_grid)\n","  grid = GridSearchCV(SVC(),param_grid, cv=5, scoring='accuracy')  #10-Fold\n","  grid.fit(X_train1, y_train1)\n","  df = pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\n","  print(df)\n","  print('\\nMax_score = ',grid.best_score_)\n","  print('Estimator : ',grid.best_estimator_)\n","  print('-------------------------------------')\n","  return (100*grid.best_score_)\n","cross_acc=[]\n","for i in range(0,9):\n","  print('S'+str(i+1)+'_500.mat')\n","  para = parameters(X_train[i],y_train[i])\n","  cross_acc.append(para)\n","print('Cross validation accuracy:  %f %%' %np.mean(cross_acc))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YHoQsSKoaWfc","executionInfo":{"status":"ok","timestamp":1643786757935,"user_tz":-420,"elapsed":3770,"user":{"displayName":"ANHQUAN NGUYEN","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05612799208685658067"}},"outputId":"43606e5a-8851-4634-e067-cac284433fc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["S1_500.mat\n","In Datasets, 90 % is Train data and 10 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[ 5.32810233e-05  1.72448745e-04 -2.86716078e-04 ... -6.66440545e-04\n","  -4.88178560e-04 -3.66886916e-04]]\n","b =  -0.6681761931690576\n","\n","Accuracy: 100.00 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       1.00      1.00      1.00         2\n","         1.0       1.00      1.00      1.00         1\n","\n","    accuracy                           1.00         3\n","   macro avg       1.00      1.00      1.00         3\n","weighted avg       1.00      1.00      1.00         3\n","\n","(3, 16384) (3,)\n","Average Cross Validation score: 96.0\n","=========================================================\n","\n","S2_500.mat\n","In Datasets, 90 % is Train data and 10 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[-0.00014712 -0.00018127 -0.00054941 ... -0.00060888 -0.00051825\n","  -0.00032771]]\n","b =  -0.6300388397580895\n","\n","Accuracy: 100.00 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       1.00      1.00      1.00         4\n","         1.0       1.00      1.00      1.00         1\n","\n","    accuracy                           1.00         5\n","   macro avg       1.00      1.00      1.00         5\n","weighted avg       1.00      1.00      1.00         5\n","\n","(5, 16384) (5,)\n","Average Cross Validation score: 100.0\n","=========================================================\n","\n","S3_500.mat\n","In Datasets, 90 % is Train data and 10 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[-0.00102885 -0.00110775 -0.00106856 ...  0.00084303  0.00087249\n","   0.00089687]]\n","b =  -0.7140321351497192\n","\n","Accuracy: 100.00 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       1.00      1.00      1.00         4\n","         1.0       1.00      1.00      1.00         4\n","\n","    accuracy                           1.00         8\n","   macro avg       1.00      1.00      1.00         8\n","weighted avg       1.00      1.00      1.00         8\n","\n","(8, 16384) (8,)\n","Average Cross Validation score: 100.0\n","=========================================================\n","\n","S4_500.mat\n","In Datasets, 90 % is Train data and 10 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[ 0.00120827 -0.00347777  0.00164092 ...  0.00358249  0.00318008\n","   0.00611153]]\n","b =  -0.7887348113280617\n","\n","Accuracy: 100.00 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       1.00      1.00      1.00         8\n","         1.0       1.00      1.00      1.00         2\n","\n","    accuracy                           1.00        10\n","   macro avg       1.00      1.00      1.00        10\n","weighted avg       1.00      1.00      1.00        10\n","\n","(10, 16384) (10,)\n","Average Cross Validation score: 94.18300653594773\n","=========================================================\n","\n","S5_500.mat\n","In Datasets, 90 % is Train data and 10 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[ 0.00061141 -0.00409011  0.00254089 ...  0.00550217  0.00404379\n","   0.00800806]]\n","b =  -0.8154747633030719\n","\n","Accuracy: 83.33 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       0.75      1.00      0.86         6\n","         1.0       1.00      0.67      0.80         6\n","\n","    accuracy                           0.83        12\n","   macro avg       0.88      0.83      0.83        12\n","weighted avg       0.88      0.83      0.83        12\n","\n","(12, 16384) (12,)\n","Average Cross Validation score: 93.46320346320347\n","=========================================================\n","\n","S6_500.mat\n","In Datasets, 90 % is Train data and 10 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[ 0.00172401 -0.00496582  0.00314009 ...  0.00590506  0.00433184\n","   0.00894713]]\n","b =  -0.5235974146447064\n","\n","Accuracy: 100.00 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       1.00      1.00      1.00         9\n","         1.0       1.00      1.00      1.00         6\n","\n","    accuracy                           1.00        15\n","   macro avg       1.00      1.00      1.00        15\n","weighted avg       1.00      1.00      1.00        15\n","\n","(15, 16384) (15,)\n","Average Cross Validation score: 92.9846153846154\n","=========================================================\n","\n","S7_500.mat\n","In Datasets, 60 % is Train data and 40 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[ 0.00043837 -0.00234218  0.00239813 ...  0.00412352  0.00282673\n","   0.00836488]]\n","b =  -0.6876385000928724\n","\n","Accuracy: 91.18 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       0.84      1.00      0.91        32\n","         1.0       1.00      0.83      0.91        36\n","\n","    accuracy                           0.91        68\n","   macro avg       0.92      0.92      0.91        68\n","weighted avg       0.93      0.91      0.91        68\n","\n","(68, 16384) (68,)\n","Average Cross Validation score: 90.0\n","=========================================================\n","\n","S8_500.mat\n","In Datasets, 90 % is Train data and 10 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[ 0.00157352 -0.00280024  0.00178007 ...  0.00725976  0.0059935\n","   0.01255842]]\n","b =  -0.5924308768318342\n","\n","Accuracy: 100.00 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       1.00      1.00      1.00        10\n","         1.0       1.00      1.00      1.00        10\n","\n","    accuracy                           1.00        20\n","   macro avg       1.00      1.00      1.00        20\n","weighted avg       1.00      1.00      1.00        20\n","\n","(20, 16384) (20,)\n","Average Cross Validation score: 95.32773109243698\n","=========================================================\n","\n","S9_500.mat\n","In Datasets, 80 % is Train data and 20 % is Test data.\n","--------------------------------------------------------\n","\n","w =  [[ 0.00361198 -0.00313223  0.00257832 ...  0.00541868  0.00466866\n","   0.00775068]]\n","b =  -0.4309382313584169\n","\n","Accuracy: 95.45 %\n","              precision    recall  f1-score   support\n","\n","        -1.0       0.91      1.00      0.95        20\n","         1.0       1.00      0.92      0.96        24\n","\n","    accuracy                           0.95        44\n","   macro avg       0.95      0.96      0.95        44\n","weighted avg       0.96      0.95      0.95        44\n","\n","(44, 16384) (44,)\n","Average Cross Validation score: 95.34453781512606\n","=========================================================\n","\n","Average of accuracy:  96.66 %\n","Cross-Validation accuracy: 95.26 %\n"]}],"source":["from sklearn.model_selection import cross_val_score,KFold\n","from sklearn.metrics import classification_report\n","def SVM(X_train1,y_train1,X_test1,y_test1,spl2,datay,labely):\n","  print('In Datasets, %d %% is Train data and %d %% is Test data.' %(((1-spl2)*100),(spl2*100)))\n","  print('--------------------------------------------------------')\n","  clf = SVC(kernel='linear',C =1)\n","  clf.fit(X_train1,y_train1) \n","  w_sklearn = clf.coef_.reshape(-1, 1)\n","  b_sklearn = clf.intercept_[0]\n","  y_pred = clf.predict(X_test1)\n","  print('\\nw = ',w_sklearn.T)\n","  print('b = ',b_sklearn)\n","  accuracy = 100*accuracy_score(y_test1, y_pred)\n","  print(\"\\nAccuracy: %.2f %%\" %(accuracy))\n","  print(classification_report(y_test1, y_pred))\n","\n","  print(X_test1.shape, y_pred.shape)\n","  kf= KFold(n_splits= 5)  #cross-validation with 5-Fold\n","  score= 100*cross_val_score(clf,X_train1,y_train1,cv=kf)\n","  #print(\"Cross Validation Scores: {}\".format(score))\n","  print(\"Average Cross Validation score: {}\".format(score.mean()))\n","  print('=========================================================\\n')\n","  return accuracy,score.mean()\n","\n","acc = [] #Store accuracy data to ndarray\n","cracc = []  #Store Cross Validation accuracy data to ndarray\n","for i in range(0,9):\n","  print('S'+str(i+1)+'_500.mat')\n","  svm = SVM(X_train[i],y_train[i],X_test[i],y_test[i],spl1[i],D[i],L[i])\n","  acc.append(svm[0])\n","  cracc.append(svm[1])\n","print('Average of accuracy:  %.2f %%' %np.mean(acc))\n","print('Cross-Validation accuracy: %.2f %%' %np.mean(cracc))"]}]}